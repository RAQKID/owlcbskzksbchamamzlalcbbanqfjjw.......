type Method = "GET" | "POST" | "PATCH" | "DELETE" | "PUT";
type RequestBody = {
    /**
     * Anything else is allowed
     *
     * text-generation: string
     * chat-model: [role: string, message: string]
     */
    [key: string]: any;
    /**
     * Stream back text
     *
     * Default: false
     */
    stream?: boolean;
    /**
     * Return JSON or stream back an media
     *
     * Default: true
     */
    json?: boolean;
};
interface Response {
    /**
     * If an error occurs during the operation, "error" is returned, and the "output" is usually null.
     */
    error: string | null;
    /**
     * If the operation is successful, the "error" argument is null, and the "output" argument contains the result of the operation.
     *
     * When running models, the output may be a number, string, vector, audio file, or whatever the model generated
     *
     */
    output: any | null;
}

declare class Client {
    #private;
    constructor(apiKey: string, dev?: boolean, isBrowser?: boolean);
    fetch: CallableFunction;
    host: string;
    headers: {};
    request(path: string, method?: Method, body?: RequestBody, providerKey?: string): Promise<any>;
}

type Task = "audio-classification" | "automatic-speech-recognition" | "chat" | "depth-estimation" | "document-question-answering" | "feature-extraction" | "fill-mask" | "image-classification" | "image-feature-extraction" | "image-segmentation" | "image-to-text" | "mask-generation" | "object-detection" | "question-answering" | "sentence-similarity" | "summarization" | "text-classification" | "text-generation" | "text-to-audio" | "text-to-image" | "text-to-speech" | "text-to-video" | "text2text-generation" | "token-classification" | "translation" | "unconditional-image-generation" | "video-classification" | "visual-question-answering" | "zero-shot-classification" | "zero-shot-image-classification" | "zero-shot-object-detection";

interface ListModels {
    /** List models by task. */
    task?: Task;
    /** Get a single model */
    modelId?: string;
}

/**
 * API Client for interfacing with the Bytez API.
 * @param apiKey Your Bytez API key
 */
declare class Bytez$2 {
    #private;
    constructor(apiKey: string, dev?: boolean, browser?: boolean);
    list: {
        /** List your auto-scaling clusters */
        clusters: () => Promise<Response>;
        /** Lists available models, and provides basic information about each one, such as RAM required */
        models: (options?: ListModels) => Promise<Response>;
        /** List available tasks */
        tasks: () => Promise<Response>;
    };
    /**
     * Get a model - allows you to run closed and open source models
     * @param modelId The modelId, for example `openai-community/gpt2`
     * @param providerKey Optional: Closed-source model provider's API key (e.g. OpenAI key)
     */
    model: (modelId: string, providerKey?: string) => Model;
}

interface Inference {
    /** Send back pure media or JSON. Default: `true` */
    json?: boolean;
    /** Stream results back. Default: `false` */
    stream?: boolean;
    /** Maximum length of generated tokens, defaults to 20. Overrides `max_new_tokens`. */
    max_length?: number;
    /** Maximum number of tokens to generate, ignoring prompt tokens. */
    max_new_tokens?: number;
    /** Minimum sequence length, defaults to 0. Overrides `min_new_tokens`. */
    min_length?: number;
    /** Minimum number of tokens to generate, ignoring prompt tokens. */
    min_new_tokens?: number;
    /** Controls stopping condition for beam search. `true` stops at `num_beams` complete candidates; `never` continues indefinitely. */
    early_stopping?: boolean | "never";
    /** Maximum computation time in seconds. Generation completes current pass after time expires. */
    max_time?: number;
    /** Whether to use sampling instead of greedy decoding, defaults to false. */
    do_sample?: boolean;
    /** Number of beams for beam search, defaults to 1. */
    num_beams?: number;
    /** Number of groups to divide beams into for diversity, defaults to 1. */
    num_beam_groups?: number;
    /** Balances model confidence and degeneration penalty in contrastive search decoding. */
    penalty_alpha?: number;
    /** Whether to use past attentions to speed up decoding, defaults to true. */
    use_cache?: boolean;
    /** Modulates next token probabilities, defaults to 1.0. */
    temperature?: number;
    /** Keeps highest probability vocabulary tokens for top-k filtering, defaults to 50. */
    top_k?: number;
    /** Keeps smallest set of tokens summing to at least `top_p`, defaults to 1.0. */
    top_p?: number;
    /** Local typicality, keeps smallest set of most typical tokens summing to `typical_p`, defaults to 1.0. */
    typical_p?: number;
    /** Only samples tokens with conditional probability above `epsilon_cutoff`. */
    epsilon_cutoff?: number;
    /** Hybrid sampling method using `eta_cutoff` and expected token probability. */
    eta_cutoff?: number;
    /** Subtracts from a beam's score if it generates a token same as any other beam's token at that time. */
    diversity_penalty?: number;
    /** Penalty for repeated tokens, 1.0 means no penalty. */
    repetition_penalty?: number;
    /** Penalty for sequences not in input, 1.0 means no penalty. */
    encoder_repetition_penalty?: number;
    /** Length penalty applied to beam scores, promotes longer or shorter sequences based on value. */
    length_penalty?: number;
    /** Restricts all ngrams of specified size to occur only once. */
    no_repeat_ngram_size?: number;
    /** Token ids not allowed in generation. */
    bad_words_ids?: number[][];
    /** Token ids that must be included in generation. */
    force_words_ids?: number[][] | number[][][];
    /** Whether to renormalize logits after all processors, highly recommended. */
    renormalize_logits?: boolean;
    /** Custom constraints for token usage in generation. */
    constraints?: Object[];
    /** Forces specified token as first generated token. */
    forced_bos_token_id?: number;
    /** Forces specified token as last when `max_length` reached. */
    forced_eos_token_id?: number | number[];
    /** Removes invalid model outputs, can slow generation. */
    remove_invalid_values?: boolean;
    /** Applies exponentially increasing length penalty after certain token count. */
    exponential_decay_length_penalty?: number[];
    /** Suppresses tokens by setting their log probs to `-inf`. */
    suppress_tokens?: number[];
    /** Suppresses tokens at the beginning of generation. */
    begin_suppress_tokens?: number[];
    /** Forces specified tokens at certain generation indices. */
    forced_decoder_ids?: number[][];
    /** Number of sequences returned for each batch element, defaults to 1. */
    num_return_sequences?: number;
    /** Whether to return attention tensors of all layers. */
    output_attentions?: boolean;
    /** Whether to return hidden states of all layers. */
    output_hidden_states?: boolean;
    /** Whether to return prediction scores. */
    output_scores?: boolean;
    /** Whether to return a structured `ModelOutput` instead of a tuple. */
    return_dict_in_generate?: boolean;
    /** Id of the padding token. */
    pad_token_id?: number;
    /** Id of the beginning-of-sequence token. */
    bos_token_id?: number;
    /** Id of the end-of-sequence token, can be a list for multiple tokens. */
    eos_token_id?: number | number[];
    /** Restricts all encoder ngrams of specified size from repeating in decoder. */
    encoder_no_repeat_ngram_size?: number;
    /** Token id to start decoding in encoder-decoder models. */
    decoder_start_token_id?: number;
    /** Additional kwargs forwarded to the model's `generate` function or used in forward pass. */
    generation_kwargs?: Object;
}

interface Capacity {
    /**
     * The auto-scaling clusters is allowed to scale-down to this amount of instances.
     *
     * To save developers money, by default, `min` is set `0`, allowing clusters to spin down all instances when they under utilized.
     *
     * Default: `0`
     */
    min?: number;
    /**
     * The auto-scaling clusters will try to aim for `desired` number of instances.
     *
     * Note: even when you set `desired`, the auto-scaling cluster may scale-up or scale-down the number of instances. This is where your cluster begins, but depending on use, this is not the exact number of instances your cluster may have.
     *
     * Default: `1`
     */
    desired?: number;
    /**
     * The auto-scaling clusters is allowed to scale-up to this amount of instances.
     *
     * Note: if you slam an auto-scaling cluster, it may exceed the `max` number of instances temporarily by a small margin, to ensure your requests are fulfilled, though in general, the cluster instances will not exceed this number.
     *
     * Default: `1` for free users or `10` for premium users
     */
    max?: number;
}
interface Create {
    /**
     * Minutes to wait before serverless cluster spins down instances.
     *
     * By default, if an instance doesn't receive a request after `5` mins, the instance shuts down.
     *
     * Receiving a request resets this timer.
     *
     * Timeout is between 1 and 1440 minutes
     *
     * Default: `5`
     */
    timeout?: number;
    /**
     * Bytez places open-source models on fully-managed auto-scaling clusters, allowing you to run open models easily at-scale.
     *
     * By default, Bytez creates a cluster that scales from 0-1 instances for free users and from 0-10 instances for premium users. If you need more instances, let us know.
     *
     * The number of instances can be thought of as concurrency / cluster capacity.
     *
     * Developers can customize cluster capacity at creation time or at anytime by simply updating the cluster.
     */
    capacity?: Capacity;
    /**
      Keyword args to pass to model when it loads
     */
    config?: object;
}
interface Update extends Create {
    /**
     * The auto-scaling cluster can be manually turned "on" or "off"
     *
     * Turning "off" cluster will shut down all instances
     */
    status?: "on" | "off";
}
type ModelRunOutput<Stream extends boolean> = Stream extends true ? NodeJS.ReadStream & ReadableStream<Uint8Array> : Response;
interface Details {
    /** Model task - text generation, object detection, etc */
    task: Task;
    /** Model params (in billions) */
    params: Number;
    /** RAM required to run inference (in GB) */
    ramRequired: Number;
    /**
     * Depending on RAM required, models are sized from `micro` models to `super` models.
     *
     */
    meter: "micro" | "xs" | "sm" | "md" | "lg" | "xl" | "xxl" | "super" | "micro-lm" | "xs-lm" | "sm-lm" | "md-lm" | "lg-lm" | "xl-lm" | "xxl-lm" | "super-lm";
    /** The price per second to run the model */
    meterPrice: string;
}

type Bytez$1 = Bytez | Bytez$2;
declare class Model {
    #private;
    constructor(modelId: string, bytez: Bytez$1, client: Client, providerKey?: string);
    /** The modelId, for example `openai-community/gpt2` */
    id: string;
    /** Default model params */
    params: Inference | undefined;
    /** details about the model */
    details: Details;
    /**
     * For open-source models, `create` an auto-scaling cluster to run this model
     *
     * @param options Cluster configuration
     */
    create: (options?: Create) => Promise<Response>;
    /** For open-source models, `read` your cluster */
    read: () => Promise<Response>;
    /**
     * For open-source models, `update` your cluster
     *
     * @param options Cluster configuration
     */
    update: (options?: Update) => Promise<Response>;
    /** For open-source models, `delete` your cluster */
    delete: () => Promise<Response>;
    /**
     * `Run` model by passing in an `input`, and optionally passing in `params` and/or a `stream` flag.
     *
     * Execute this function in 1 of 4 ways:
  
     * 1. run(`input`) => returns JSON => { error, output }
     * 2. run(`input`, `params`) => returns JSON => { error, output }
     * 3. run(`input`, `stream`) => stream === true? returns read stream, else returns JSON
     * 4. run(`input`, `params`, `stream`) => stream === true? returns read stream, else returns JSON
     *
     * Parameters:
     * @param input - Input to pass to model (e.g., text, URL, base64).
     * @param params - models parameters object
     * @param stream - boolean
     */
    run(input?: any, params?: Inference): Promise<Response>;
    run<Stream extends boolean = false>(input?: any, stream?: Stream): Promise<ModelRunOutput<Stream>>;
    run<Stream extends boolean = false>(input?: any, params?: Inference, stream?: Stream): Promise<ModelRunOutput<Stream>>;
}

/**
 * API Client for interfacing with the Bytez API.
 * @param apiKey Your Bytez API key
 */
declare class Bytez {
    #private;
    constructor(apiKey: string, dev?: boolean, browser?: boolean);
    list: {
        /** List your auto-scaling clusters */
        clusters: () => Promise<Response>;
        /** Lists available models, and provides basic information about each one, such as RAM required */
        models: (options?: ListModels) => Promise<Response>;
        /** List available tasks */
        tasks: () => Promise<Response>;
    };
    /**
     * Get a model - allows you to run closed and open source models
     * @param modelId The modelId, for example `openai-community/gpt2`
     * @param providerKey Optional: Closed-source model provider's API key (e.g. OpenAI key)
     */
    model: (modelId: string, providerKey?: string) => Model;
}

export { Bytez as B, Bytez$2 as default };
